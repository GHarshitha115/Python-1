from requests import get
url = 'http://www.imdb.com/search/title?release_date=2017&sort=num_votes,desc&page=1'
response = get(url)
print(response.text[:500])
-------------------------OUTPUT---------------------------------------------
<!DOCTYPE html>
<html
    xmlns:og="http://ogp.me/ns#"
    xmlns:fb="http://www.facebook.com/2008/fbml">
    <head>
         
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="apple-itunes-app" content="app-id=342792525, app-argument=imdb:///?src=mdot">



        <script type="text/javascript">var IMDbTimer={starttime: new Date().getTime(),pt:'java'};</script>

<script>
    if (typeof uet == 'function') {
      uet("bb", "LoadTitle")
________________________________________________________________________________________

import requests
payload = {'username':'mahesh','password':'iBHubs'}
request = requests.post('https://w3schools.com/post',data=payload)
print(request.url)
#data_obj = request
#print(type(data_obj))
#result = textwrap.wrap(data_obj,width=100)
#print(result)

________________________________________________________________________________________

import requests
from bs4 import BeautifulSoup

r =  """
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;
charset=iso-8859-1">
<title>An example of HTML page</title>
</head>
<body>
<h2>This is an example HTML page</h2>
<p>
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc at nisi velit,
aliquet iaculis est. Curabitur porttitor nisi vel lacus euismod egestas. In hac
habitasse platea dictumst. In sagittis magna eu odio interdum mollis. Phasellus
sagittis pulvinar facilisis. Donec vel odio volutpat tortor volutpat commodo.
Donec vehicula vulputate sem, vel iaculis urna molestie eget. Sed pellentesque
adipiscing tortor, at condimentum elit elementum sed. Mauris dignissim
elementum nunc, non elementum felis condimentum eu. In in turpis quis erat
imperdiet vulputate. Pellentesque mauris turpis, dignissim sed iaculis eu,
euismod eget ipsum. Vivamus mollis adipiscing viverra. Morbi at sem eget nisl
euismod porta.</p>
<p><a href="https://www.w3resource.com/html/HTML-tutorials.php">Learn HTML from
w3resource.com</a></p>
<p><a href="https://www.w3resource.com/css/CSS-tutorials.php">Learn CSS from 
w3resource.com</a></p>
</body>
</html>
"""
soup = BeautifulSoup(r,'html.parser')
'''
for p in soup.find_all('p'):
  para = p.text
  print(para)'''
print(type(soup.find_all('p')))
print(soup.find_all('p'))

soup = BeautifulSoup(r,'html.parser')

#print(soup.find('a').attrs['href'])
print(soup.a['href'])


________________________________To Get The Required Attrs in HTML________________________________________________________

import requests
from bs4 import BeautifulSoup

r = requests.get('https://python.org').text

soup = BeautifulSoup(r,'lxml')

urls = []
for i in soup.find_all('li'):
    a = i.find('a')
    urls.append(a.attrs['href'])
    
print(urls)
________________________________________________________________________________________

import requests
from bs4 import BeautifulSoup

r = requests.get('https://python.org').text

soup = BeautifulSoup(r,'lxml')
print(soup.find_all('li'))
print(soup.find_all('h1'))
print(soup.find_all('h2'))
print(soup.find_all('h3'))

_______________________________Print Headings in the HTML File_________________________________________________________

import requests
from bs4 import BeautifulSoup

r = requests.get('https://python.org').text

soup = BeautifulSoup(r,'lxml')

for heading in soup.find_all(['h1','h2','h3']):
    print(heading.name+' '+heading.text)

#print(soup.find_all('h1','h2','h3'))


OUTPUT:

h1 


h1 Functions Defined
h1 Compound Data Types
h1 Intuitive Interpretation
h1 Quick & Easy to Learn
h1 All the Flow You�d Expect
h2 Get Started
h2 Download
h2 Docs
h2 Jobs
h2 Latest News
h2 Upcoming Events
h2 Success Stories
h2 Use Python for�
h2 
>>> Python Enhancement Proposals (PEPs): The future of Python is discussed here.
 RSS

h2 
>>> Python Software Foundation

________________________________________________________________________________________
